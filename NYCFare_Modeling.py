import pandas as pd#from sklearn import ensemblefrom sklearn.preprocessing import StandardScalerimport numpy as npimport matplotlib.pyplot as pltfrom sklearn.metrics import mean_squared_errorfrom sklearn.model_selection import train_test_split#from sklearn.model_selection import GridSearchCVimport xgboost as xgbfrom geopy.distance import geodesic#datatypes for column data. Float16 for reduced mem consumptiontypes = {'fare_amount': 'float16',         'pickup_longitude': 'float32',         'pickup_latitude': 'float32',         'dropoff_longitude': 'float32',         'dropoff_latitude': 'float32',         'passenger_count': 'uint8',         'hour_of_day': 'uint8',         'year': 'uint8',         'quarter':'uint8',         'month':'uint8',         'day_of_year': 'uint8',         'day_of_week': 'uint8'         }cols = ['fare_amount', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude',        'dropoff_latitude', 'passenger_count',        'hour_of_day','year','quarter','month',        'day_of_year','day_of_week'        ]train_set = pd.read_csv("s3://supersickdatascience/allclean_traindata_nodist.csv", dtype = types, usecols = cols, index_col=False)train_set["distance"] = train_set[["pickup_latitude","pickup_longitude","dropoff_latitude","dropoff_longitude"]].apply(lambda x: geodesic((x[0],x[1]),(x[2],x[3])).miles,axis=1)#Feature engineering repeated for test set. Can make this a function. #abs(var-x) is done to make the feature periodic. i.e hours day 1 and 7 lose information about their proximity unless made periodic.eval_set = pd.read_csv("s3://supersickdatascience/test.csv", infer_datetime_format=True, parse_dates=["pickup_datetime"])eval_set["hour_of_day"] = eval_set["pickup_datetime"].apply(lambda x: abs(x.hour-12))eval_set["year"] = eval_set["pickup_datetime"].apply(lambda x: x.year)eval_set["day_of_year"] = eval_set["pickup_datetime"].apply(lambda x: abs(x.dayofyear-182))eval_set["day_of_week"] = eval_set["pickup_datetime"].apply(lambda x: abs(x.dayofweek-3))eval_set["month"] = eval_set["pickup_datetime"].apply(lambda x: abs(x.month-6))eval_set["quarter"] = eval_set["pickup_datetime"].apply(lambda x: abs(x.quarter-2))eval_set["distance"] = eval_set[["pickup_latitude","pickup_longitude","dropoff_latitude","dropoff_longitude"]].apply(lambda x: geodesic((x[0],x[1]),(x[2],x[3])).miles,axis=1)eval_set_final = eval_set.copy(deep=True)eval_set_key = eval_set_final["key"]eval_set_final = eval_set_final.drop(["key","pickup_datetime"],axis=1)#Backup CSV. eval_set_final.to_csv("eval_set_fixedup.csv")#splitting data. Train on all `data because google test set is provided. train_set_split, test_set_split, train_set_fare, test_set_fare = train_test_split(train_set, train_set_fare, test_size=0.000001, random_state=42)#transform data to -1 to 1. scaler = StandardScaler()scaler.fit(train_set_split)train_set_split = scaler.transform(train_set_split.values)test_set_split = scaler.transform(test_set_split.values)#Prep making XGData Matrix -- unique data storage for XGBoost. dtrain = xgb.DMatrix(train_set_split, label=train_set_fare)dtest = xgb.DMatrix(test_set_split, label = test_set_fare)parameters = {'silent':0, 'learning_rate':.25, 'nthread':-1,'max_depth':5 ,'subsample':1.0,               'verbose_eval':1, 'gamma':0.5,'colsample_bytree': 1.0,'min_child_weight': 10,               'random_state': 7}evallist = [(dtesteval , 'eval'), (dtrain, 'train')]XGB = xgb.XGBRegressor()#Modelbst = xgb.train(parameters, dtrain,num_rounds, evallist)bst.save_model('XGB_NYFare.model')#plot of feature importances.featImport = xgb.plot_importance(bst)featImport.savefig("feature_importance.png")#Predict outputs to submit.deval_final = xgb.DMatrix(eval_set_final)predicted_output = bst.predict(deval_final)fixed_output = pd.DataFrame(data = predicted_output, columns=['fare_amount'])fixed_output = pd.concat((eval_set_key,fixed_output),axis=1)fixed_output.to_csv("predicted_outputs.csv",index=False,date_format=str)